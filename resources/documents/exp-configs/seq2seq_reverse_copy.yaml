# To run this experiment, you need to
#     0. put this file in /path/to/SFUTranslate/resources directory
#     1. run "cd /path/to/SFUTranslate/src && python -m translate.learning.trainer  <name of this file placed in the resources directory>"
#     2. The test Bleu score of the test set is expected to be above 99 and the test loss should be around 0.01
reader:
    dataset:
        type: dummy_parallel
        buffer_size: 1000
        max_length: 50
        dummy:
            min_len: 10
            max_len: 50
            vocab_size: 1000
            train_samples: 40000
            test_samples: 1000
            dev_samples: 100
    vocab:
        bos_word: '<s>'
        eos_word: '</s>'
        pad_word: '<pad>'
        unk_word: '<unk>'
        space_word: '<space>'
        bpe_separator: '@@'
trainer:
    model:
        type: seq2seq
        bsize: 64
        init_val: 0.1
        best_model_path: null
        beam_size: 1
        tfr: 1.001
        bienc: true
        hsize: 256
        nelayers: 1
        ndlayers: 1
        ddropout: 0.1
    optimizer:
        name: adam
        lr: 1e-3
        gcn: 5
        epochs: 30
        save_best_models: true
        early_stopping_loss: 0.01
    experiment:
        name: seq2seq_reverse_copy