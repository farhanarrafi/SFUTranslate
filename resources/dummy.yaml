reader:
    dataset:
        type: dummy_s2s # possible values [dummy_s2s | dummy_lm | dummy_transformer]
        buffer_size: 1000
        max_length: 128 # for word-level it's better to be around 50-60, for bpe level around 128
        source_lang: fr
        target_lang: en
        working_dir: ../../../resources/iwslt
        train_file_name: train
        test_file_name: test
        dev_file_name: dev
        granularity: !!python/object/apply:translate.readers.constants.ReaderLevel [1]
        dummy:
            min_len: 8
            max_len: 50
            vocab_size: 96
            train_samples: 40000
            test_samples: 3000
            dev_samples: 1000
    vocab:
        bos_word: '<s>'
        eos_word: '</s>'
        pad_word: '<pad>'
        unk_word: '<unk>'
        bpe_separator: '@@'
trainer:
    model:
        ####### universal configurations
        type: seq2seq # possible values [seq2seq | rnnlm | transformer]
        bsize: 64 # size of the training sentence batches
        init_val: 0.1 # the value to range of which random variables get initiated
        ####### seq2seq/rnnlm configurations
        tfr: 1.001 # teacher forcing ratio
        bienc: true # bidirectional encoding
        hsize: 256 # hidden state size of RNN layers
        nelayers: 1 # number of hidden layers in encoder
        ndlayers: 1 # number of hidden layers in decoder
        ddropout: 0.1 # the dropout probability in the decoder
        ####### transformer configurations
        N: 6 # number of encoder/decoder layers
        d_model: 512 # size of each encoder/decoder layer
        d_ff: 2048 # size of intermediate layer in feedforward sub-layers
        h: 8 # number of heads
        dropout: 0.1 # the dropout used in encoder/decoder model parts
    optimizer:
        name: adadelta
        lr: 1.0
        gcn: 5 # grad clip norm
        epochs: 15
        save_best_models: true
        early_stopping_loss: 0.01 # if the model reaches a loss below this value, the training will not continue anymore
        ####### transformer configurations
        warmup_steps: 400
        lr_update_factor: 1
        needs_warmup: false
    experiment:
        name: 'dummy'